{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import nltk\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['studentAnswer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'[a-z]+')\n",
    "\n",
    "def preprocess(document):\n",
    "    document = document.lower() # Convert to lowercase\n",
    "    words = tokenizer.tokenize(document) # Tokenize\n",
    "    words = [w for w in words if not w in stop_words] # Removing stopwords\n",
    "    # Lemmatizing\n",
    "    for pos in [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]:\n",
    "        words = [wordnet_lemmatizer.lemmatize(x, pos) for x in words]\n",
    "    return \" \".join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['studentAnswer'] = df['referenceAnswer'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentAnswer = df[\"studentAnswer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.studentAnswer, df.accuracy, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape: {}\".format(x_train.shape), end='\\n')\n",
    "print(\"y_train shape: {}\".format(y_train.shape), end='\\n\\n')\n",
    "print(\"x_test shape: {}\".format(x_test.shape), end='\\n')\n",
    "print(\"y_test shape: {}\".format(y_test.shape), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and transforming the training data to a document-term matrix using TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\n",
    "X_train_tfidf = tfidf.fit_transform(x_train)\n",
    "print(\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) #1722\n",
    "print(\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])\n",
    "\n",
    "#  DecisionTreeClassifier()\n",
    "lr =  DecisionTreeClassifier()\n",
    "lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "sorted_coef_index = lr.coef_[0].argsort()\n",
    "print('\\nTop 10 features with the smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Top 10 features with the largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEvaluation(predictions):\n",
    "    '''\n",
    "    Print model evaluation to predicted result \n",
    "    '''\n",
    "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "    #print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(tfidf.transform(x_test))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfm = confusion_matrix(y_test, predictions)\n",
    "plt.imshow(cfm, interpolation='nearest')\n",
    "\n",
    "for i, j in itertools.product(range(cfm.shape[0]), range(cfm.shape[1])):\n",
    "    plt.text(j, i, cfm[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\")\n",
    "\n",
    "plt.ylabel('True label (Recall)')\n",
    "plt.xlabel('Predicted label (Precision)')\n",
    "plt.title('Logistic Regression with TFIDF | Confusion Matrix')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
