{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDp2V10eUpV4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "le = LabelEncoder() \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9FSqRlAU0YE"
   },
   "outputs": [],
   "source": [
    "!pip  install -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z50-j-b_VEmp"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rKKUmdaUpWE"
   },
   "outputs": [],
   "source": [
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('/content/drive/MyDrive/glove.6B.200d.txt'), encoding = \"utf-8\") as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "    for line in f:\n",
    "        values = line.split() #split at space\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32') #numpy.asarray()function is used when we want to convert input to an array.\n",
    "        word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYXtkRz4VI5V"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuU10gd0UpWI"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('/content/drive/MyDrive/Dataset.csv',encoding='utf-8')\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df ['accuracy'] = label_encoder.fit_transform(df['accuracy'])\n",
    "df.accuracy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58-wE_OfUpWL"
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder \n",
    "#le = LabelEncoder()\n",
    "#df['DX']= le.fit_transform(df['DX']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pd1drp4UpWN"
   },
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wfZrml7UpWQ"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0zcP4s3UpWT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test= pd.read_csv(\"/content/drive/MyDrive/Dataset.csv\")\n",
    "\n",
    "test1=test.drop('Unnamed: 0',axis = 1 )\n",
    "y_test = test1.accuracy                           \n",
    "X_test= test1.drop('accuracy',axis = 1 )\n",
    "for j in range(0,len(test)):\n",
    "    df=df[df.index!=test['Unnamed: 0'][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-TL3ZqxUpWX"
   },
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5oRd31sUpWa"
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCvD5q0_UpWc"
   },
   "outputs": [],
   "source": [
    "y_train = df.accuracy                           \n",
    "X_train= df.drop('accuracy',axis = 1 )\n",
    "#print(df.head())\n",
    "print(df['accuracy'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3k8PSoOCUpWf"
   },
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBeTjDeUF3vB"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size = 0.3, random_state=119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTQYxR8xUpWi"
   },
   "outputs": [],
   "source": [
    "# Encoding the Dependent Variable\n",
    "labelencoder_y = LabelEncoder()\n",
    "y_train = labelencoder_y.fit_transform(y_train)\n",
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1yWL0ZFUpWl"
   },
   "outputs": [],
   "source": [
    "y_test = labelencoder_y.fit_transform(y_test)\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIR0-gouUpWp"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXagrbr4UpWs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG3ZLto4UpWu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "SFold = StratifiedKFold(n_splits=10, random_state=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBCvyWQKUpWw"
   },
   "outputs": [],
   "source": [
    "num_words = 20000\n",
    "embedding_dim = 200\n",
    "max_length = 32\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = 'OOV'\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train['studentAnswer'])\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train['studentAnswer'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "padded_train = pad_sequences(sequences=train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print('Total unique tokens generated: ',len(word_index))\n",
    "print('Shape of padded train tensor: ', padded_train.shape)\n",
    "\n",
    "#tokenizer = Tokenizer(num_words=num_words, oov_token=oov_tok)\n",
    "#tokenizer.fit_on_texts(testing_sentences)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test['studentAnswer'])\n",
    "padded_test = pad_sequences(sequences=test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print('Shape of padded test tensor: ', padded_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XL4QXKW2UpW0"
   },
   "outputs": [],
   "source": [
    "num_words = min(20000, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "embeddings = []\n",
    "for word, i in word_index.items():\n",
    "    if i<20000:\n",
    "        embeddings = word2vec.get(word)\n",
    "        if embeddings is not None:\n",
    "            embedding_matrix[i] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "che1KzFwUpW3"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Gz_4WvdUpW6"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    num_units_min  =  50\n",
    "    num_units_max  =  500\n",
    "    num_units_step =  20\n",
    "\n",
    "    dropout_min  =  .1\n",
    "    dropout_max  =  0.9\n",
    "    dropout_step =  0.1\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    #e = Embedding(input_dim=num_words, output_dim = embedding_dim,weights=[embedding_matrix],input_length=max_length, trainable=False)\n",
    "    model.add(layers.Embedding(input_dim=num_words, output_dim = embedding_dim,weights=[embedding_matrix],input_length=max_length, trainable=False))\n",
    "    \n",
    "\n",
    "    \n",
    "    model.add(layers.LSTM(units=hp.Int('unit1',  min_value=num_units_min,\n",
    "                                                 max_value=num_units_max,\n",
    "                                                 step=num_units_step) ,kernel_regularizer=keras.regularizers.l2(hp.Choice('reg_rate2',values=[0.01, 0.05, 0.1, .2,.3,.4,.5])),                  \n",
    "                                          return_sequences = True))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_1',min_value=dropout_min,\n",
    "                                      max_value=dropout_max,\n",
    "                                      step=dropout_step) ) )\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 1, 1)):\n",
    "        model.add(layers.LSTM(units=hp.Int('unitfor_'+ str(i),\n",
    "                                           min_value=num_units_min,\n",
    "                                           max_value=num_units_max,\n",
    "                                         step=num_units_step),\n",
    "                                        activation='relu',  kernel_regularizer=keras.regularizers.l2(hp.Choice('reg_rate2',values=[0.01, 0.05, 0.1, .2,.3,.4,.5])),                  \n",
    "                                          return_sequences = True))\n",
    "        model.add(layers.Dropout(hp.Float('dropoutfor_'+ str(i), \n",
    "                                    min_value=dropout_min,\n",
    "                                    max_value=dropout_max,\n",
    "                                      step=dropout_step)))\n",
    "    model.add(layers.LSTM(units=hp.Int('unit4',  min_value=num_units_min,\n",
    "                                                 max_value=num_units_max,\n",
    "                                                 step=num_units_step),\n",
    "                                                 activation='relu',  kernel_regularizer=keras.regularizers.l2(hp.Choice('reg_rate3',values=[0.01, 0.05, 0.1,.2,.3,.4,.5])),                  \n",
    "                                                 return_sequences = False))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_4',min_value=dropout_min,\n",
    "                                      max_value=dropout_max,\n",
    "                                      step=dropout_step) ) ) \n",
    "    model.add(layers.Flatten())\n",
    "    '''for i in range(hp.Int('num_layers', 1, 5)):\n",
    "        model.add(layers.Dense(units=hp.Int('unitsdense_'+ str(i),\n",
    "                                            min_value=50,\n",
    "                                            max_value=300,\n",
    "                                            step=10),\n",
    "                                            activation='relu',\n",
    "                              kernel_regularizer=keras.regularizers.l2(hp.Choice('reg_rate4',values=[0.01, 0.05, 0.1,.2,.3,.4,.5]))))   \n",
    "        model.add(layers.Dropout(hp.Float('dropoutdense_'+ str(i), \n",
    "                                      min_value=dropout_min,\n",
    "                                      max_value=dropout_max,\n",
    "                                      step=dropout_step)))'''\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',values=[ 1e-4])),\n",
    "       loss='binary_crossentropy', \n",
    "       metrics=['acc',f1_m,precision_m, recall_m])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ou2nKqSpUpW9"
   },
   "outputs": [],
   "source": [
    "X_t, X_val, y_t, y_val = train_test_split(\n",
    "     padded_train, y_train, test_size=0.10, stratify= y_train, shuffle=True, random_state=None) \n",
    "len(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA4Lshw6UpW_"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_acc',\n",
    "    max_trials=10,\n",
    "    project_name='/content/drive/My Drive/Colab Notebooks/lstm333')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWzk6NGVUpXB"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_acc', patience=30, verbose=1)\n",
    "callback_list = [ early_stopping ]\n",
    "\n",
    "# split training data into stratified train/dev sets\n",
    "\n",
    "h=tuner.search(X_t, y_t,\n",
    "             epochs=10,\n",
    "             batch_size=100, \n",
    "             callbacks=callback_list, validation_data=(X_val,y_val) )\n",
    "            \n",
    "model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sd_rXwnrUpXF"
   },
   "outputs": [],
   "source": [
    "X_t, X_val, y_t, y_val = train_test_split(\n",
    "     padded_train, y_train, test_size=0.10, stratify= y_train, shuffle=True, random_state=None) \n",
    "len(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gNC2xFMUpXJ"
   },
   "outputs": [],
   "source": [
    "history=model.fit(X_t,\n",
    "                  y_t,\n",
    "                  batch_size=100,\n",
    "                  epochs=10, validation_data=(X_val,y_val))\n",
    "# evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gw69j0KlUpXM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mJawW9CUpXP"
   },
   "outputs": [],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8jZ1OYeUpXS"
   },
   "outputs": [],
   "source": [
    "y_pred = (model.predict(padded_test) >= 0.5).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rqe72VhcUpXV"
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0GVudzzUpXY"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFexyOg_UpXe"
   },
   "outputs": [],
   "source": [
    "AccuracyTrain=[]\n",
    "SDAccuracyTrain=[]\n",
    "PrecisionTrain=[]\n",
    "SDPrecisionTrain=[]\n",
    "RecallTrain=[]\n",
    "SDReacllTrain=[]\n",
    "F1Train=[]\n",
    "SDF1Train=[]\n",
    "AccuracyTest=[]\n",
    "PrecisionTest=[]\n",
    "RecallTest=[]\n",
    "F1Test=[]\n",
    "r=0\n",
    "for i in range(0,10):\n",
    "        \n",
    "        \n",
    "        X_t, X_val, y_t, y_val = train_test_split(padded_train, y_train, test_size=0.10, stratify= y_train)\n",
    "        h=model.fit(X_t, y_t, epochs=10, batch_size=100,  verbose=1,validation_data=(X_val,y_val))\n",
    "\n",
    "        AccuracyTrain.append(round(100*np.mean(h.history['val_acc']),2))\n",
    "        SDAccuracyTrain.append(round(100*np.std(h.history['val_acc']), 2))\n",
    "        PrecisionTrain.append(round(100*np.mean(h.history['val_precision_m']),2))\n",
    "        SDPrecisionTrain.append(round(100*np.std(h.history['val_precision_m']), 2))\n",
    "\n",
    "        RecallTrain.append(round(100*np.mean(h.history['val_recall_m']),2))\n",
    "        SDReacllTrain.append(round(100*np.std(h.history['val_recall_m']), 2))\n",
    "        F1Train.append(round(100*np.mean(h.history['val_f1_m']),2))\n",
    "        SDF1Train.append(round(100*np.std(h.history['val_f1_m']), 2))\n",
    "        y_pred = (model.predict(padded_test) >= 0.5).astype(\"int\")\n",
    "       \n",
    "        Accurcy_Test= accuracy_score(y_test,y_pred)\n",
    "        Precision_Test=precision_score(y_test, y_pred,average='weighted')\n",
    "\n",
    "        Recall_Test=recall_score(y_test, y_pred, average='weighted')\n",
    "        F1_Test=f1_score(y_test, y_pred, average='weighted') \n",
    "\n",
    "        AccuracyTest.append(round(100*Accurcy_Test, 2))\n",
    "        PrecisionTest.append(round(100*Precision_Test, 2))\n",
    "        RecallTest.append(round(100*Recall_Test, 2))\n",
    "        F1Test.append(round(100*F1_Test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUM_lr5nUpXi"
   },
   "outputs": [],
   "source": [
    "ReultofTest=pd.DataFrame([])\n",
    "ReultofTest=ReultofTest.append({'AccuracyTest' : round(np.mean(AccuracyTest),2),'PrecisionTest':round(np.mean(PrecisionTest),2),\n",
    "             'RecallTest' : round(np.mean(RecallTest),2),'F1Test':round(np.mean(F1Test),2)}, ignore_index=True)\n",
    "ReultofTest.to_csv(\"LSTMLTrain.csv\")\n",
    "ReultofTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFRImhANUpXl"
   },
   "outputs": [],
   "source": [
    "AT= str(round(np.mean(AccuracyTrain),2))\n",
    "SDATrain=str(round(np.mean(SDAccuracyTrain),2))\n",
    "Accuracy_SD=AT +\"±\" +SDATrain\n",
    "Accuracy_SD\n",
    "PT= str(round(np.mean(PrecisionTrain),2))\n",
    "SDPTrain=str(round(np.mean(SDPrecisionTrain),2))\n",
    "PrecisionTrain_SD=PT +\"±\" +SDPTrain\n",
    "PrecisionTrain_SD\n",
    "RT= str(round(np.mean(RecallTrain),2))\n",
    "SDReacllTrain=str(round(np.mean(SDReacllTrain),2))\n",
    "RecallTrain_SD=RT +\"±\" +SDReacllTrain\n",
    "RecallTrain_SD\n",
    "FT= str(round(np.mean(F1Train),2))\n",
    "SDFTrain=str(round(np.mean( SDF1Train),2))\n",
    "FTrain_SD=FT +\"±\" +SDFTrain\n",
    "FTrain_SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4nEQPtkUpXn"
   },
   "outputs": [],
   "source": [
    "finaltrain = pd.DataFrame([])\n",
    "\n",
    "finaltrain = finaltrain.append({'AccuracyTrain' : Accuracy_SD,\n",
    "                                'PrecisionTrain':PrecisionTrain_SD,\n",
    "                                'RecallTrain':RecallTrain_SD,  \n",
    "                                'F1Train':FTrain_SD, \n",
    "                               \n",
    "                               } , ignore_index=True)\n",
    "finaltrain .to_csv(\"warapper_result.csv\")\n",
    "finaltrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4F7y1cZUpXt"
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8J-xVnIUpXv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "LSTM three layer.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
